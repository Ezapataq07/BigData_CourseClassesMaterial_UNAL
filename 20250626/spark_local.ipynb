{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/UNAL_Logosimbolo.svg/583px-UNAL_Logosimbolo.svg.png\" alt=\"\" width=\"1280\" height=\"300\" /></p>\n"
      ],
      "metadata": {
        "id": "TWniqiCNOAhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# APACHE SPARK**\n",
        "\n",
        "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1280px-Apache_Spark_logo.svg.png\" alt=\"\" width=\"150\" height=\"100\" /></p>\n",
        "\n",
        "\n",
        "\n",
        "Apache Spark ‚Ñ¢ es un motor multilenguaje para ejecutar ingenier√≠a de datos, ciencia de datos y aprendizaje autom√°tico en m√°quinas de un solo nodo o cl√∫steres.\n",
        "\n",
        ">  ‚ö†Ô∏è **ADVERTENCIA:** Si ven el s√≠mbolo `!`, significa que ese comando est√° pensado para ejecutarse desde un notebook, donde se usa para enviar instrucciones al sistema operativo.\n",
        "Si van a ejecutar el mismo comando directamente desde la consola, deben remover el signo de exclamaci√≥n."
      ],
      "metadata": {
        "id": "VbLeZD07OFrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CREACI√ìN DE ARCHIVO LOCAL**"
      ],
      "metadata": {
        "id": "vnnrEMBDOzxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z9ozZoIN9Vw",
        "outputId": "67c2085e-074a-46dc-ece3-e9e33052f9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing datos.csv\n"
          ]
        }
      ],
      "source": [
        "%%writefile datos.csv\n",
        "nombre,edad,ciudad\n",
        "ana,28,medellin\n",
        "luis,34,bogota\n",
        "mario,22,cali\n",
        "laura,30,barranquilla\n",
        "carlos,27,manizales"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **INSTALACI√ìN DE SPARK**"
      ],
      "metadata": {
        "id": "2cRVuTf5O6gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark --quiet"
      ],
      "metadata": {
        "id": "_alGVfqnO48f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **VERIFICACI√ìN DE VERSI√ìN**"
      ],
      "metadata": {
        "id": "iVxb--TOQCdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pyspark --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RzanT-qQGKj",
        "outputId": "9efa9e18-128a-41bc-facf-9f33a8ab5bfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.27\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **INTERACCI√ìN ELEMENTAL CON SPARK**"
      ],
      "metadata": {
        "id": "L4pxgIfcQQkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPORTAR LIBRER√çAS**"
      ],
      "metadata": {
        "id": "N9iic5_QQU9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "fhJdisabQbmT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CREACI√ìN DE CONTEXTO**\n",
        "\n",
        "En Spark, crear un contexto significa inicializar el entorno que permite ejecutar operaciones sobre datos.\n",
        "\n",
        "Explicado brevemente para principiantes:\n",
        "\n",
        "* Spark necesita un **contexto** para empezar a trabajar, algo as√≠ como abrir una sesi√≥n de trabajo.\n",
        "\n",
        "* Ese contexto le dice a Spark c√≥mo y d√≥nde procesar los datos (por ejemplo, localmente, en cluster, etc.).\n",
        "\n",
        "* En versiones modernas, usamos `SparkSession`, que reemplaza al antiguo `SparkContext`."
      ],
      "metadata": {
        "id": "v0brELdIQdcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "  .appName(\"mi primer ejemplo con pyspark\") \\\n",
        "  .getOrCreate()"
      ],
      "metadata": {
        "id": "YUM9wNLwQyJK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "xu7D15aWRFol",
        "outputId": "995b45e1-9b2b-4b72-c86f-7d01a04f30bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ef312c94650>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://92b47759546f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>mi primer ejemplo con pyspark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CREACI√ìN DE DATOS DUMMY**"
      ],
      "metadata": {
        "id": "RmyjTafSRSlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(\n",
        "  [\n",
        "    {\"nombre\": \"ana\", \"edad\": 28},\n",
        "    {\"nombre\": \"luis\", \"edad\": 34},\n",
        "    {\"nombre\": \"mario\", \"edad\": 22}\n",
        "  ]\n",
        ")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU1bhJl6RSIe",
        "outputId": "46247722-ee3f-4e2a-e4b5-ee2ecde2932b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|edad|nombre|\n",
            "+----+------+\n",
            "|  28|   ana|\n",
            "|  34|  luis|\n",
            "|  22| mario|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LECTURA DE ARCHIVOS**\n",
        "\n",
        "Spark siempre trabaja con **sistemas de archivos distribuidos**, no con el sistema local por defecto. Por esta raz√≥n, **la ruta del archivo debe incluir el esquema que indica el tipo de almacenamiento**.\n",
        "\n",
        "Adem√°s, ten en cuenta que **el formato de archivo por defecto en Spark es Parquet**, un formato columnar optimizado para rendimiento.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wx77Ri5sRhx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LECTURA HDFS**\n",
        "\n",
        "Hadoop Distributed File System\n",
        "\n",
        "\n",
        "\n",
        "```bash\n",
        "hdfs://namenode:9000/user/data/archivo.parquet\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "WWhmj-cJSOTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LECTURA DATABRICKS (DBFS)**\n",
        "\n",
        "\n",
        "```bash\n",
        "dbfs:/mnt/datalake/archivo.parquet\n",
        "```\n"
      ],
      "metadata": {
        "id": "Tw3zAegsScK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LECTURA LOCAL**\n",
        "\n",
        "\n",
        "```bash\n",
        "file:///ruta/local/archivo.parquet\n",
        "```\n",
        "\n",
        "> üí° **Nota:** Usar rutas locales (`file://`) no es recomendable en producci√≥n, ya que Spark est√° dise√±ado para procesar datos en entornos distribuidos."
      ],
      "metadata": {
        "id": "ngN3Z0yYShd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **EJEMPLO LECTURA LOCAL**"
      ],
      "metadata": {
        "id": "LYg6ye9ZSsC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read.csv(\n",
        "    \"file:///content/datos.csv\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        "  )"
      ],
      "metadata": {
        "id": "x0Ngga-qS0Db"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEUt_5dnS5Ku",
        "outputId": "670026ab-e34c-4d64-99c5-e31287d1c85f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- nombre: string (nullable = true)\n",
            " |-- edad: integer (nullable = true)\n",
            " |-- ciudad: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQKd6gQoTBQX",
        "outputId": "22274e17-6984-41c3-cd9d-ad4b7a9ddcb5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+------------+\n",
            "|nombre|edad|      ciudad|\n",
            "+------+----+------------+\n",
            "|   ana|  28|    medellin|\n",
            "|  luis|  34|      bogota|\n",
            "| mario|  22|        cali|\n",
            "| laura|  30|barranquilla|\n",
            "|carlos|  27|   manizales|\n",
            "+------+----+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}