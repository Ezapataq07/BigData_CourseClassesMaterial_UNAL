{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a53003da-9bfb-46c9-b13f-0da4e53cd9bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/UNAL_Logosimbolo.svg/583px-UNAL_Logosimbolo.svg.png\" alt=\"\" width=\"1280\" height=\"300\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8866804b-ac79-407d-a806-7e3891e54c15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# USER DEFINE FUNCTION\n",
    "\n",
    "UDF (User Defined Function) in Spark is a way to extend the functionality of Spark SQL by allowing you to write custom functions and apply them to data in Spark DataFrames or Datasets.\n",
    "\n",
    "These functions can be written in languages like Python, Scala, or Java, depending on the Spark environment you are using. UDFs allow you to perform complex operations that Spark SQL does not natively support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75c44a29-f1be-4717-a195-8a10b588e486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## BENEFITS\n",
    "\n",
    "* Customization: Allows defining custom logic not available in Spark’s built-in functions.\n",
    "* Flexibility: Provides the ability to implement your own business logic when built-in functions aren’t sufficient.\n",
    "* Reuse: UDFs can be reused across multiple queries and operations.\n",
    "* Compatibility: Enables the integration of external libraries or custom algorithms.\n",
    "Interoperability: Supports extending Spark’s functionality using languages like Python or Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31acfd1c-4892-4503-a956-740e7e0dcf27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DISADVANTAGES\n",
    "\n",
    "* **Performance Overhead**: UDFs run on a per-record basis and bypass Spark’s optimization, leading to slower performance compared to built-in operations.\n",
    "* **Limited Parallelization**: UDFs are not optimized for parallel execution, reducing performance on large datasets.\n",
    "* **No Catalyst Optimization**: UDFs cannot leverage the Catalyst query optimizer, missing out on optimizations like predicate pushdown.\n",
    "* **Serialization Costs**: Data must be serialized and deserialized between JVM and Python, adding overhead.\n",
    "* **Error Handling**: Debugging UDFs is harder than built-in functions due to limited visibility into execution plans.\n",
    "* **Memory Control**: UDFs in non-Java environments (like Python) fall outside Spark’s memory management, potentially leading to inefficiencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f716dfbb-3f98-4021-8f2f-14d81a8fd06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CREATE\n",
    "\n",
    "To do that you must have a elemental knoldge of data types and python programing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d258762-301e-4494-b86e-58ace92cfea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql.functions import udf, StringType, ArrayType, col, lower\n",
    "\n",
    "elements = [\n",
    "    {\"id\": 1, \"name\": \"July\", \"age\": 34, \"salary\": 550, \"role\": \"admin\"},\n",
    "    {\"id\": 1, \"name\": \"July\", \"age\": 34, \"salary\": 550, \"role\": \"admin\"},\n",
    "    {\"id\": 2, \"name\": \"Gabriel\", \"age\": 29, \"salary\": 720, \"role\": \"developer\"},\n",
    "    {\"id\": 3, \"name\": \"Luis\", \"age\": 42, \"salary\": 610, \"role\": \"developer\"},\n",
    "    {\"id\": 4, \"name\": \"John\", \"age\": 51, \"salary\": 890, \"role\": \"manager\"},\n",
    "    {\"id\": 5, \"name\": \"Daniel\", \"age\": 27, \"salary\": 480, \"role\": \"developer\"},\n",
    "    {\"id\": 6, \"name\": \"Mary\", \"age\": 38, \"salary\": 700, \"role\": \"admin\"},\n",
    "    {\"id\": 7, \"name\": \"Monica\", \"age\": 33, \"salary\": 460, \"role\": \"tester\"},\n",
    "    {\"id\": 8, \"name\": \"Andrea\", \"age\": 45, \"salary\": 680, \"role\": \"admin\"},\n",
    "    {\"id\": 9, \"name\": \"Sebastian\", \"age\": 31, \"salary\": 530, \"role\": \"developer\"},\n",
    "    {\"id\": 10, \"name\": \"Johana\", \"age\": 26, \"salary\": 410, \"role\": \"tester\"},\n",
    "    {\"id\": 11, \"name\": None, \"age\": 26, \"salary\": None, \"role\": \"tester\"},\n",
    "    {\"id\": 12, \"name\": \"Juan\", \"age\": 45, \"salary\": 680, \"role\": None},\n",
    "]\n",
    "df = spark.createDataFrame(elements)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b70bff1a-249e-42de-b93c-9a26dbe3a5bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TEMPORAL ELEMENTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e901693-2085-4d77-b362-6f3caa7b270a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_list_letters(value: str) -> List[str]:\n",
    "    if value is None:\n",
    "        return []\n",
    "    return list(value.lower())\n",
    "\n",
    "get_list_letters(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6113f2fa-6f2f-4412-ada3-5bdf1757609a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create udf\n",
    "my_udf = udf(get_list_letters, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40cb1408-045c-4753-ab3e-cd78a36b3469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower\n",
    "df.select(\n",
    "    col(\"name\"),\n",
    "    my_udf(col(\"name\")).alias(\"letters\")\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8569a2f2-323a-43b1-8a74-ccd78521d05d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DECORATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f6b4b1-db3b-402b-9293-cb98a5648cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType()))\n",
    "def get_list_letters(value):\n",
    "    if value is None:\n",
    "        return []\n",
    "    return list(str(value).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c83abdb-3dbc-4253-95db-d08b5c98ddad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(\n",
    "    col(\"name\"),\n",
    "    get_list_letters(\n",
    "        col(\"name\")\n",
    "    ).alias(\"letters\")\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c1c4ae-df41-4546-b872-41eaf6333d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### REGISTER\n",
    "\n",
    "Registering a UDF in Spark means giving your function a name so it can be used directly in SQL queries, just like built-in Spark functions.\n",
    "\n",
    "Benefits:\n",
    "* Use in SQL: Call your UDF inside spark.sql() queries.\n",
    "* BI Integration: Make the UDF available in dashboards and reporting tools connected to Spark.\n",
    "* Reusable: Easily call the function by name across multiple queries and notebooks.\n",
    "* Organized: Helps structure your project like a database with user-defined functions.\n",
    "\n",
    "But Note:\n",
    "* Not persistent: Registered UDFs are lost when you stop or restart your Spark session or cluster.\n",
    "* No performance boost: Registering doesn’t make UDFs faster or optimized.\n",
    "* Still a black box: Spark’s optimizer (Catalyst) cannot optimize registered UDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3334e77d-0f92-4b0b-b91c-2f8ad1ec9e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe64300-3302-412b-8916-c23f242519d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_letters(value: str) -> List[str]:\n",
    "    if value is None:\n",
    "        return []\n",
    "    return list(value.lower())\n",
    "\n",
    "get_list_letters(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ebdb26-7e22-4beb-9ae1-b3a8ea91e592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.udf.register(\"get_cert_udf\", get_letters, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4620331b-2a89-45de-9f8f-21ca92b22d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### USING SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54eb70e0-0aa3-4c72-8322-7188710f7645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"my_test_udf_sl\")\n",
    "spark.sql(\" SELECT get_cert_udf(name) FROM my_test_udf_sl\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1a28f9-95af-488b-811a-9c700e96f007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### USING PYSPARK SELECTEXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6dd69e-8eab-44c0-929d-a97656a9e99b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.selectExpr(\"get_cert_udf(name) as lettesr_list\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "004.udf",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
