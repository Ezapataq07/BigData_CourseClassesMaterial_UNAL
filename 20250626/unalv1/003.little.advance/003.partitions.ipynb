{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68327d84-474e-4e8a-a63f-848e07a86a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/UNAL_Logosimbolo.svg/583px-UNAL_Logosimbolo.svg.png\" alt=\"\" width=\"1280\" height=\"300\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffb734a0-c83e-4caa-b012-3914cbb9be3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PARTITIONS\n",
    "\n",
    "In PySpark, `repartition()` is used to change the number of partitions of a DataFrame. It reshuffles the data across the cluster to create exactly the number of partitions you specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dfec881-dc9e-4f97-842d-ef94e94c5742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## WHY IS REPARTITION IMPORTANT?\n",
    "\n",
    "* Optimizing parallelism: More partitions can improve parallelism and speed up large jobs.\n",
    "* Avoiding data skew: You can spread out uneven data more evenly across partitions.\n",
    "* Preparing for joins: Matching partitioning before joins can reduce shuffling and improve performance.\n",
    "* Efficient writing: You can control the number of output files when writing data (e.g., avoid too many small files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4d1b29d-705a-4006-989c-c186fd097b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## KEY CONCEPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1629fa5e-d084-4294-9139-f1f42989ac4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SHUFLE\n",
    "\n",
    "In Apache Spark, a shuffle is a costly operation where data is redistributed across partitions and nodes in the cluster. It happens when actions like groupBy(), join(), or distinct() require data to be rearranged to complete the task.\n",
    "\n",
    "Why is shuffle important?\n",
    "Shuffle is slow and resource-heavy because it involves reading and writing data between nodes and storage. This increases both execution time and memory usage.\n",
    "\n",
    "Common operations that cause shuffle:\n",
    "  * `groupBy()` : Moves data to group by key.\n",
    "  * `join()` : Combines data based on a key, across partitions.\n",
    "  * `distinct()` : Removes duplicates, requiring data movement.\n",
    "  * `sort()` / `orderBy()`: Requires total ordering, so data must be shuffled across partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae3c266f-94c0-449a-84e5-85e475c69932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PARTITION DATAFRAME\n",
    "\n",
    "Each partition in Spark is a unit of work that can be processed in parallel.  \n",
    "But it is not the same as a core.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "```\n",
    "Spark divides a DataFrame into partitions.\n",
    "|_ Each partition is processed by a task.\n",
    "  |_ A core can execute one task at a time.\n",
    "```\n",
    "\n",
    "- If you have 8 partitions and 4 cores → Spark will process 4 partitions at the same time, then the other 4.\n",
    "- If you have 8 partitions and 8 cores → Spark can process all 8 in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a38a9de-0f71-4236-85be-9059e9192798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DIFFERENCE BETWEEN REPARTITION AND COALESCE\n",
    "\n",
    "- **`repartition()`** reshuffles all the data and evenly distributes it across the specified number of partitions.  \n",
    "  This ensures balanced partitions but involves a full shuffle, which can be expensive in terms of performance.\n",
    "\n",
    "- **`coalesce()`** reduces the number of partitions by merging existing ones without moving much data.  \n",
    "  It is faster and more efficient, especially when decreasing partitions.  \n",
    "  However, it does not rebalance the data, so partition sizes may become uneven.\n",
    "\n",
    "**Tip:**  \n",
    "Use `repartition()` when you need evenly distributed data (e.g., before heavy joins or aggregations).  \n",
    "Use `coalesce()` when you simply want to reduce partitions (e.g., before writing data) without the cost of a full shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb51a35c-629c-400c-a9f1-f249d63fda46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be1843c-eba6-40f6-9a57-23cdd58d05a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "elements = [\n",
    "    {\"id\": 1, \"name\": \"July\", \"age\": 34, \"salary\": 550, \"role\": \"admin\"},\n",
    "    {\"id\": 1, \"name\": \"July\", \"age\": 34, \"salary\": 550, \"role\": \"admin\"},\n",
    "    {\"id\": 2, \"name\": \"Gabriel\", \"age\": 29, \"salary\": 720, \"role\": \"developer\"},\n",
    "    {\"id\": 3, \"name\": \"Luis\", \"age\": 42, \"salary\": 610, \"role\": \"developer\"},\n",
    "    {\"id\": 4, \"name\": \"John\", \"age\": 51, \"salary\": 890, \"role\": \"manager\"},\n",
    "    {\"id\": 5, \"name\": \"Daniel\", \"age\": 27, \"salary\": 480, \"role\": \"developer\"},\n",
    "    {\"id\": 6, \"name\": \"Mary\", \"age\": 38, \"salary\": 700, \"role\": \"admin\"},\n",
    "    {\"id\": 7, \"name\": \"Monica\", \"age\": 33, \"salary\": 460, \"role\": \"tester\"},\n",
    "    {\"id\": 8, \"name\": \"Andrea\", \"age\": 45, \"salary\": 680, \"role\": \"admin\"},\n",
    "    {\"id\": 9, \"name\": \"Sebastian\", \"age\": 31, \"salary\": 530, \"role\": \"developer\"},\n",
    "    {\"id\": 10, \"name\": \"Johana\", \"age\": 26, \"salary\": 410, \"role\": \"tester\"},\n",
    "    {\"id\": 11, \"name\": None, \"age\": 26, \"salary\": None, \"role\": \"tester\"},\n",
    "    {\"id\": 12, \"name\": \"Juan\", \"age\": 45, \"salary\": 680, \"role\": None},\n",
    "]\n",
    "df = spark.createDataFrame(elements)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52578203-b47f-4298-95d0-2fc0a70ae891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REPARTITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd04fc57-9849-454d-acf1-ae49be9192f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f65e2c8-7b5c-4a2b-969f-c04f837d65a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "380e5d1e-8724-472f-863e-07482bc3d76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdcd451b-10a1-463e-856d-7f98b4725db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### REPARTITION ONLY\n",
    "\n",
    "This redistributes the rows randomly across 10 partitions. It doesn’t consider any specific column, so the data can end up in any partition. This is useful when you just want to balance the workload across more or fewer partitions, for example:\n",
    "\n",
    "- When your DataFrame is highly unbalanced.\n",
    "- When you need more parallelism (to speed up certain operations).\n",
    "\n",
    "Disadvantage:\n",
    "\n",
    "It doesn’t guarantee that related data will stay together. For instance, if you later perform a `groupBy` on a column, Spark might need to shuffle the data across the network because that column wasn’t considered during partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4247521-0712-4adb-aa9f-1ec460f1e7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_part_only = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903e24d1-dc75-47a5-8024-7a46868e2501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_part_only.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99c581b-78c9-4272-915a-0298a5d26cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# with defaul partitions\n",
    "df.groupBy(\"role\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c372b73-4852-4c79-8ee0-096c2dce93b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# with repartitions\n",
    "df_part_only.groupBy(\"role\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97663495-08f1-493e-a6b5-bcc54ccc2f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_part_only = df.repartition(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a64867c7-e62c-4fef-9471-c3f12f76155d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### REPARTITION WITH COLUMN\n",
    "\n",
    "Here, Spark distributes the rows based on the specified column. It uses a hash of the column to decide which partition each row goes to. This is very useful when:\n",
    "\n",
    "- You want to perform operations like `groupBy` or `join` using that column.\n",
    "- You want to minimize shuffle during those steps since the data is already grouped by that key.\n",
    "\n",
    "Advantage:\n",
    "\n",
    "- Reduces data movement in subsequent operations on the key column.\n",
    "- Improves performance if your workflow involves many aggregations or joins on that column.\n",
    "\n",
    "Recomendation:\n",
    "\n",
    "If you’re going to do `df.groupBy(\"key_column\").agg(...)`, it's better to partition by \"key_column\" beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b04d74-9ed5-4462-914e-70d19dd5181e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_part_columns = df.repartition(2, \"role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c30248-59f2-47ac-a58e-4e06da35a570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_part_columns.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab32772-4772-4988-b95f-622a3a7c19a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_part_columns.groupBy(\"role\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d63514-4f2c-44a9-8b9f-db1565c90c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_part_columns = df.repartition(1, \"role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22216875-73f8-474f-834b-76c7e6344a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## COALESCE\n",
    "\n",
    "In PySpark, `coalesce()` is a function that reduces the number of partitions in a DataFrame to a specified number.  \n",
    "It is a transformation operation that merges existing partitions, minimizing data movement.  \n",
    "Unlike `repartition()`, `coalesce()` does not rebalance the partitions, which may result in partitions of uneven sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7cca9c6-04de-4188-b750-5bec069d83f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e6153d-87b5-4957-9162-dd195d4154d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_coal = df.coalesce(1)\n",
    "df_coal.groupBy(\"role\").count().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65c09712-f2c9-4046-8808-6d87e11c0dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PROBLEM WITH SMALL FILES\n",
    "\n",
    "equal number of partitions == number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33576da-3e08-4ef0-89e6-4ddc4cec27e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# original df\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0e78f0-26ad-4516-b176-0083cac6f2b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# grouping and checking partitins\n",
    "cls_partitions = df.groupBy(\"role\").count()\n",
    "print(cls_partitions.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac29fa26-0901-4c92-b0d6-960d00b5e3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode(\"ignore\").save(\"file:///tmp/x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6fc092e-4079-484e-912b-0491d7139038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -l /tmp/x.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b1416f8-1a63-4ac1-8251-8addc20a36b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.coalesce(1).write.format(\"csv\").mode(\"ignore\").save(\"file:///tmp/x.csv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195d1db4-5f15-4066-9a6c-d7d905ba23a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -l /tmp/x.csv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c79e18f4-d103-4171-8c4b-9ce7a287f27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PARTITION BY\n",
    "\n",
    "When you use `partitionBy` while writing data in Spark, it organizes the output files into separate folders based on the distinct values of one or more columns.\n",
    "This is known as storage partitioning and is different from in-memory partitions used during processing.\n",
    "\n",
    "Why use partitionBy?\n",
    "\n",
    "It significantly improves read and filter performance because Spark can skip entire folders during queries — a technique called partition pruning. \n",
    "\n",
    "**NOTE: Partition pruning is when Spark avoids reading entire partitions (folders) because it knows it doesn't need them based on your filter.**\n",
    "\n",
    "Ideal when you frequently filter or query by specific columns (e.g., country, date, region).\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1072/1*n853xsKLRFCqxd3FyMyMdg.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e3a369e-7162-404f-b713-d008dd89dea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26956df5-83de-430e-98b6-4d46d3514f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode(\"ignore\").partitionBy(\"role\").save(\"file:///tmp/part.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79ce41b-b9c3-4eb5-adc8-8d027bc6f3e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -R /tmp/part.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00432931-e1de-40fd-ba2c-4240f29a8896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MULTI COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31016901-0e6c-4cc4-b3e3-5feec572fc38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode(\"ignore\").partitionBy(\"role\", \"name\").save(\"file:///tmp/part.csv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12cdc04e-e2cf-46d8-977b-fe62f0c46e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -R /tmp/part.csv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd9a8826-0c13-483c-894f-7c08c3474e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PARTITION PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76d9acd-e280-4083-be92-c08dc53ee1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.csv(\"file:///tmp/part.csv\", sep=\",\", ).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9f5b32-0ba7-4993-955d-66d88bfe42b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.csv(\"file:///tmp/part.csv/role=admin\", sep=\",\", ).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8507d1d8-dbf7-42de-a28c-e61d429f4a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CHOOSING BAD PARTITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae665bff-e85c-4702-8b15-3f22b0762090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode(\"ignore\").partitionBy(\"id\").save(\"file:///tmp/part.csv3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85383081-7669-4d42-9cd0-270d8585c685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -R /tmp/part.csv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "588b0712-5223-4bfa-9e55-c2d267732350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e601edd0-5c9e-44a3-bf1f-cc18147f0507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### maxPartitionBytes\n",
    "Max size per partition when reading files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f97f7e7-0472-45c2-ac34-c37a62be74ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "size = spark.conf.get(\"spark.sql.files.maxPartitionBytes\")\n",
    "print(f\"{int(size[:-1]) / (1024 * 1024)} MB\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3121590259974487,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "003.partitions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
