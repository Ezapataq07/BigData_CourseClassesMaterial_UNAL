{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d52df83c-fc67-42d7-86d3-7f056bd0ccfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/UNAL_Logosimbolo.svg/583px-UNAL_Logosimbolo.svg.png\" alt=\"\" width=\"1280\" height=\"300\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e407849a-d247-447b-a6b0-64c283903062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SPARK SESSION\n",
    "\n",
    "`SparkSession` is the entry point to programming with Spark using the DataFrame and Dataset API. It allows you to read data, execute SQL queries, manage configurations, and interact with Spark clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe123811-9575-4fdf-acbb-20c01d8ab147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## AUTOMATICALLY\n",
    "In data platforms like **Databricks**, a SparkSession is automatically created and available via the `spark` variable, you donâ€™t need to initialize it manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9184348d-5ef2-432b-a76e-2f81658f6ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MANUAL\n",
    "\n",
    "In other tools or environments like **Cloudera**, you need to manually create the SparkSession before using it. Here's a basic example:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be0ceffb-81c9-49bd-90a2-5867a6cf39ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SPARK METHODS\n",
    "\n",
    "- `spark.version` : returns the current Spark version  \n",
    "- `spark.sparkContext` : access to the underlying SparkContext  \n",
    "- `spark.udf` : register and use user-defined functions (UDFs)  \n",
    "- `spark.conf` : get or set Spark configurations  \n",
    "- `spark.catalog` : interact with Spark catalog (databases, tables, functions)  \n",
    "- `spark.read` : read data from CSV, JSON, Parquet, etc.  \n",
    "- `spark.readStream` : read streaming data sources  \n",
    "- `spark.range()` : create a DataFrame with a range of numbers  \n",
    "- `spark.sql()` : run SQL queries using Spark SQL  \n",
    "- `spark.createDataFrame()` : create a DataFrame from a local object or RDD  \n",
    "- `spark.stop()` : stop the active Spark session\n",
    "\n",
    "Note: Note: We will use almost all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f9e378a-0bcc-419b-b665-793b2fc86fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SPARK INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343b8894-5e73-43fc-952a-4391dedc0b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09eadf0c-5d06-4cee-a156-e86a6d705b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SPARK VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff8db6d-1d03-4b90-b723-9002f75a859e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "926df3c6-2aa1-4917-8130-4738eaf75f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SPARK CONFIG\n",
    "You can **get** or **set** Spark configuration parameters using `spark.conf`.\n",
    "\n",
    "**SET**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "```\n",
    "\n",
    "**GET**\n",
    "```python\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "```\n",
    "\n",
    "[SPARK CONFIGURATION](https://spark.apache.org/docs/latest/configuration.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f34c76-a9c8-469c-8977-41a7f5b8dd26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark.conf.get(\"spark.app.name\"))                          # name of the Spark application\n",
    "print(spark.conf.get(\"spark.master\"))                            # cluster manager to connect to (local, yarn, etc.)\n",
    "print(spark.conf.get(\"spark.executor.memory\"))                   # memory per executor (e.g., 2g)\n",
    "#print(spark.conf.get(\"spark.executor.cores\"))                    # number of cores per executor\n",
    "#print(spark.conf.get(\"spark.driver.memory\"))                     # memory available to the driver\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))            # number of partitions used during shuffle operations\n",
    "#print(spark.conf.get(\"spark.default.parallelism\"))               # default number of tasks for parallel operations\n",
    "print(spark.conf.get(\"spark.sql.warehouse.dir\"))                 # directory for Spark SQL managed tables\n",
    "#print(spark.conf.get(\"spark.serializer\"))                        # serialization class used\n",
    "print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))    # threshold for automatic broadcast join (in bytes)\n",
    "print(spark.conf.get(\"spark.hadoop.fs.s3a.block.size\"))          # block size used when reading from S3 (Hadoop connector)\n",
    "size = int(spark.conf.get(\"spark.hadoop.fs.s3a.block.size\"))\n",
    "print(f\"block size: {size / (1024 * 1024):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "001.spark.variable",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
