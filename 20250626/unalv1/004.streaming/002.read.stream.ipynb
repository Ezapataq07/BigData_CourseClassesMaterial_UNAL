{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be8521b0-94f8-4814-906b-d1a2fc165234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/UNAL_Logosimbolo.svg/583px-UNAL_Logosimbolo.svg.png\" alt=\"\" width=\"1280\" height=\"300\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "667362e7-9f78-4ae5-a5f4-5b0ea818a96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# READ STREAMING DATA\n",
    "\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. \n",
    "\n",
    "nternally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "970e495c-3355-40bc-a1c7-f3e00ced1641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SPARK STREAMING TYPES ENGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f97eaa8d-74c4-465a-905d-4f26ee51fec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DISCRETIZED STREAMS (DISCONTINUED)\n",
    "\n",
    "DStreams are the original streaming model in Spark, introduced in Spark Streaming (before Spark 2.0).\n",
    "\n",
    "It works by breaking the incoming stream into small batches (micro-batches). Each batch is processed as an RDD (Resilient Distributed Dataset).\n",
    "\n",
    "DStreams use RDD-based APIs, which are lower-level and more manual.\n",
    "\n",
    "It's called Discretized because it converts a continuous stream into discrete chunks (batches) every few seconds.\n",
    "\n",
    "In simple words:\n",
    "DStreams = Old Spark Streaming, works with batches of RDDs behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b52cd31-f79c-4875-b00b-5316add3db2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### STRUCTURED STREAMING (CURRENT)\n",
    "\n",
    "Structured Streaming is the modern and recommended streaming engine in Spark (introduced in Spark 2.0).\n",
    "\n",
    "It works using DataFrames and SQL APIs.\n",
    "\n",
    "Even though it also uses micro-batches internally, it presents the stream as a continuous table that keeps getting updated as new data arrives.\n",
    "\n",
    "It supports event-time processing, windowing, joins, and SQL operations on streaming data — much more powerful and easier to write.\n",
    "\n",
    "In simple words:\n",
    "Structured Streaming = Streaming with DataFrames and SQL, modern, powerful, and simpler to use.\n",
    "\n",
    "\n",
    "**KEY CONCEPT: UNBOUNDED TABLE**\n",
    "![unbounded table](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)\n",
    "\n",
    "\n",
    "**BETTER UNDERSTANDING**\n",
    "![unbounded table](https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb14b57f-d106-435b-9ca4-1ab755b24775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SPARK.READ\n",
    "\n",
    "`spark.read` is the entry point in Apache Spark to load batch data (static data that doesn’t change while you're processing it).\n",
    "\n",
    "**Sintax** \n",
    "\n",
    "\n",
    "```python\n",
    "df = spark.read.format(\"format_name\") \\\n",
    "    .option(\"key\", \"value\") \\\n",
    "    .load(\"path or source\")\n",
    "\n",
    "\n",
    "df = df.transformations(...)\n",
    "```\n",
    "\n",
    "\n",
    "* `.format() `→ defines the data source (e.g., csv, parquet, json, jdbc, etc.)\n",
    "* `.option()` → sets specific options (like header, delimiter, etc.)\n",
    "* `.load()` → loads the data from file path or source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e49d82-85fe-4f2b-85d2-8803f3169b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### FORMATS\n",
    "\n",
    "The `.format()` method in Spark tells Spark which data source to connect to when reading data in streaming mode. It’s like saying: \"Where are the incoming data coming from that I want to process in streaming?\"\n",
    "\n",
    "```python\n",
    "df = spark.readStream.format(\"kafka\").load()\n",
    "```\n",
    "\n",
    "support mucho others, but these are the common integrations\n",
    "\n",
    "\n",
    "| **Format**   | **Description**                                               |\n",
    "|--------------|---------------------------------------------------------------|\n",
    "| `kafka`    | Read streaming data from an Apache Kafka topic.               |\n",
    "| `rate`     | Simulated data stream (generates rows per second).            |\n",
    "| `socket`   | Read streaming text data from a TCP socket (useful for demos).|\n",
    "| `delta`    | Read streaming data from a Delta Lake table.                  |\n",
    "| `parquet`  | Read new Parquet files arriving in a folder.                  |\n",
    "| `csv`      | Read new CSV files arriving in a folder in streaming mode.    |\n",
    "| `json`     | Read new JSON files arriving in a folder in streaming mode.   |\n",
    "| `pubsub`   | Read data from Google Cloud Pub/Sub.                          |\n",
    "| `kinesis`  | Read data from Amazon Kinesis.                                |\n",
    "| `eventhubs`| Read data from Azure Event Hubs.                              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b1b3cd2-26a6-43d2-b92e-7893d9bbf7d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### READING SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9c6aa02-5aa9-4d25-9464-ab0104809c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### KAFKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d214492-4ee4-4217-924b-8afac971778a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_kafka = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"my_topic\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19bc43ac-c886-4f07-8650-38d6b89b9d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0c50324-f566-4191-98e3-817820c70989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/path/to/file.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65046a12-32cc-4f12-b282-137716a6c516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb684ee-6f89-47b1-b561-23d28f3d2a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_folder = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/path/to/my_folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc8e8f8e-fdd9-49db-80ff-33ca387d69ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### SYNTHETIC DATA\n",
    "\n",
    "\n",
    "RateStreamSource is a streaming source that generates consecutive numbers with timestamp that can be useful for testing and PoCs.\n",
    "\n",
    "[for more details, click here](https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-RateStreamSource.html)\n",
    "\n",
    "| **Name**         | **Default Value**         | **Description**                              |\n",
    "|------------------|---------------------------|----------------------------------------------|\n",
    "| `numPartitions`  | (default parallelism)     | Number of partitions to use                  |\n",
    "| `rampUpTime`     | 0 (seconds)               | Time to ramp up before reaching max speed    |\n",
    "| `rowsPerSecond`  | 1                         | Number of rows to generate per second (must be > 0) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f211d583-2f05-4615-b288-6d7e9ef54774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9597f31c-1d3a-4f8e-ad0d-dcddc3525c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(stream_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286cf7d1-2e1a-4a19-875e-d179ea31647b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfs_default = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
    "display(dfs_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2e75e0-337a-4bcb-af7f-c5039ab34074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col, window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a966d68-9eb0-48c8-b2b5-ce16a37385bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dfs_default.groupBy(window(col(\"timestamp\"), \"5 seconds\")).agg(count(\"*\").alias(\"rows_per_5_seconds\")).orderBy(\"window\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf70c7a7-9f8e-4ab8-b9f6-ddb8076f2cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### CUSTOM DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99710c11-beac-4edb-9f21-6979937583d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3625bcf-08d5-4e18-9794-7af757509993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_simulated = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 2).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2aace10-c37b-4841-b8ce-cd255eb40f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_simulated = df_simulated.withColumn(\"date_generated\", current_timestamp())\\\n",
    "    .withColumn(\"product_id\", (rand() * 10).cast(\"int\")) \\\n",
    "    .withColumn(\"quantity\", (rand() * 5 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"price_usd\", round(rand() * 100 + 10, 2)) \\\n",
    "    .withColumn(\"total_usd\", col(\"quantity\") * col(\"price_usd\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d3426e-30e2-414a-b989-ec71c04fd83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f626bc71-c244-4d02-872c-f0cd4d9b6f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_simulated.groupBy(window(col(\"timestamp\"), \"20 seconds\")).agg(count(\"*\").alias(\"rows_per_10_seconds\")).orderBy(\"window\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60e65d94-3b22-4fc9-8e4c-6ebdd096eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### FROM JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e386fa74-ee3b-4565-b2e4-2096996e9659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfs_json = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 2).load()\n",
    "df_simulated = dfs_json.withColumn(\"date_generated\", current_timestamp())\\\n",
    "    .withColumn(\"product_id\", (rand() * 10).cast(\"int\")) \\\n",
    "    .withColumn(\"quantity\", (rand() * 5 + 1).cast(\"int\")) \\\n",
    "    .withColumn(\"price_usd\", round(rand() * 100 + 10, 2)) \\\n",
    "    .withColumn(\"total_usd\", col(\"quantity\") * col(\"price_usd\"))\n",
    "\n",
    "json_df = df_simulated.withColumn(\n",
    "    \"json_value\",\n",
    "    to_json(\n",
    "        struct(\n",
    "            col(\"date_generated\"),\n",
    "            col(\"product_id\"),\n",
    "            col(\"quantity\"),\n",
    "            col(\"price_usd\"),\n",
    "            col(\"total_usd\")\n",
    "        )\n",
    "    )\n",
    ").select(\"date_generated\", \"json_value\")\n",
    "\n",
    "display(json_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15298cb4-6f9b-443b-ab14-41d9afb2df4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### FROM FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e4851d-b634-4765-8a92-bf1c22483ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# \n",
    "base_path: str = \"/mnt/data/streaming_lab2\"\n",
    "\n",
    "dbutils.fs.rm(base_path, recurse=True)\n",
    "dbutils.fs.mkdirs(base_path)\n",
    "print(f\"folder: {base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cafd4dba-70fa-4724-99a6-0de2158fe547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import random as rd\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_events(rows):\n",
    "    events = []\n",
    "    for _ in range(rows):\n",
    "        event = {\n",
    "            \"event_time\": datetime.now().isoformat(),\n",
    "            \"user_id\": rd.randint(1, 5),\n",
    "            \"product_id\": rd.randint(100, 105),\n",
    "            \"quantity\": rd.randint(1, 3),\n",
    "            \"price\": rd.uniform(10.0, 100.0)\n",
    "        }\n",
    "        events.append(event)\n",
    "    return events\n",
    "\n",
    "\n",
    "for _ in range(3):\n",
    "    events = generate_events(10)\n",
    "    file_path = f\"{base_path}/events_{uuid.uuid4()}.json\"\n",
    "    \n",
    "    with open(f\"/dbfs{file_path}\", \"w\") as f:\n",
    "        for event in events:\n",
    "            f.write(json.dumps(event) + \"\\n\")\n",
    "    \n",
    "    print(f\"file -> {file_path} written.\")\n",
    "    time.sleep(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555fc836-9a38-4650-abac-e02d6d6d1c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df = (\n",
    "    spark.readStream\n",
    "    .schema(\"event_time timestamp, user_id int, product_id int, quantity int, price double\")\n",
    "    .json(base_path)\n",
    ")\n",
    "\n",
    "display(stream_df)  # Visualiza en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ed731d-a147-4532-abe4-e8c1d87190ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])\n",
    "# schema(schema)\n",
    "# Read stream with schema\n",
    "stream_df = (\n",
    "    spark.readStream.format(\"json\")\n",
    "    .schema(\"event_time timestamp, user_id int, product_id int, quantity int, price double\")\n",
    "    .load(base_path)\n",
    ")\n",
    "stream_df_with_file = stream_df.withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "display(stream_df_with_file)  # Visualiza en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f57184e-d547-4df6-9b7b-7fd9927a14ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col, window, current_timestamp, rand, round, to_json, struct, input_file_name\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, DoubleType\n",
    "\n",
    "from pyspark.sql.functions import window, sum as _sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04d5ff0f-c57a-4772-958c-dfb319610207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### WATERMARK\n",
    "\n",
    "Generally speaking, when working with real-time streaming data there will be delays between event time and processing time due to how data is ingested and whether the overall application experiences issues like downtime. \n",
    "\n",
    "In Spark, the concept of watermark is used in Structured Streaming to handle late data, especially when events do not arrive in the expected order or with delays. The watermark helps Spark manage the delay of data and allows for real-time processing without waiting indefinitely.\n",
    "\n",
    "Supported Interval Types:\n",
    "\n",
    "* Seconds (seconds or s)\n",
    "* Minutes (minutes or m)\n",
    "* Hours (hours or h)\n",
    "* Days (days or d)\n",
    "* Milliseconds (milliseconds or ms)\n",
    "\n",
    "In another words: allows to have a time tolerance for late messages of another batch\n",
    "\n",
    "TIP\n",
    "\n",
    "Watermark SOLO funciona si usas ventanas de tiempo.\n",
    "El watermark sirve para manejar datos retrasados.\n",
    "\n",
    "Pero Spark necesita algo que cierre ventanas para decidir cuándo descartar los datos atrasados.\n",
    "\n",
    "Esas \"cosas\" que se cierran son las ventanas de tiempo (usadas en groupBy(window(...))).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dedb4003-1f44-42e3-ac78-663fcf324627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 2).load()\n",
    "\n",
    "water_df = (\n",
    "  stream_df\n",
    "  .withWatermark(\"timestamp\", \"30 seconds\")  # tolera datos con 30 seg de retraso\n",
    "  .groupBy(\n",
    "    window(col(\"timestamp\"), \"10 seconds\"),\n",
    "  )\n",
    "  .agg(\n",
    "      _sum(col(\"value\") ).alias(\"total_value\"),\n",
    "  )\n",
    "  .orderBy(\"window\")\n",
    ")\n",
    "\n",
    "display(water_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d61377-1278-4199-808a-9b41fdb9fecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "water_df2 = (\n",
    "  stream_df\n",
    "  .withWatermark(\"timestamp\", \"30 seconds\")\n",
    ")\n",
    "display(water_df2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "002.read.stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
