{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb61eda-32c3-4b8b-b7df-0c59ce880068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# WRITE SPARK STREAM\n",
    "\n",
    "In PySpark Structured Streaming, .writeStream is the method used to define how and where you want to output your streaming data.\n",
    "\n",
    "| Method             | Description                                                        |\n",
    "|--------------------|---------------------------------------------------------------------|\n",
    "| format()           | Sets the output format (e.g., \"parquet\", \"json\", \"console\", \"kafka\"). |\n",
    "| option()           | Sets options specific to the output format (e.g., path, checkpoint location). |\n",
    "| outputMode()       | Defines how data is written: \"append\", \"complete\", or \"update\".     |\n",
    "| partitionBy()      | (Optional) Partitions output files by specified columns (works with file sinks). |\n",
    "| trigger()          | Sets the trigger interval for micro-batches (e.g., every \"10 seconds\"). |\n",
    "| queryName()        | Assigns a name to the query for Spark UI monitoring.                |\n",
    "| foreachBatch()   | (Advanced) Processes each micro-batch as a DataFrame (best for databases, APIs). |\n",
    "| foreach() | (Advanced) Processes each row individually (best for row-based sinks like NoSQL). |\n",
    "| start()            | Starts the streaming query.                                         |\n",
    "| awaitTermination() | Waits until the query is stopped.                                   |\n",
    "| stop()             | Stops the streaming query.                                          |\n",
    "| status             | Shows the current status (e.g., active).                            |\n",
    "| recentProgress     | Shows recent progress reports.                                      |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e9d3457-58ea-4873-8520-d9c769cd6f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SOME DETAIL METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac0f57c2-f5c4-4169-a4a9-7fdbb0dcb6fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### FORMAT\n",
    "\n",
    "Common options:\n",
    "\n",
    "| Format     | Description                                         |\n",
    "|------------|-----------------------------------------------------|\n",
    "| **parquet**  | Reads/writes streaming data as Parquet files.      |\n",
    "| **json**     | Reads/writes streaming data as JSON files.         |\n",
    "| **csv**      | Reads/writes streaming data as CSV files.          |\n",
    "| **delta**    | Reads/writes streaming data to Delta Lake tables (ACID). |\n",
    "| **kafka**    | Reads/writes streaming data from/to Kafka topics.  |\n",
    "| **console**  | Writes streaming data to console (debugging).      |\n",
    "| **memory**   | Writes streaming data to an in-memory table (debugging). |\n",
    "| **socket**   | Reads streaming text data from a TCP socket (testing). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f49aad3c-91b6-4c39-a8d7-0ea510ea45b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OPTIONS\n",
    "\n",
    "| Option Key              | Description                                       | Applies To             |\n",
    "|-------------------------|---------------------------------------------------|------------------------|\n",
    "| **path**                | Output or input path for files.                   | File formats, Delta    |\n",
    "| **checkpointLocation**  | Path for saving checkpoints (mandatory in streaming). | All streaming sinks    |\n",
    "| **header**              | Whether CSV files have headers (true/false).      | CSV format             |\n",
    "| **delimiter**           | Delimiter character for CSV.                      | CSV format             |\n",
    "| **kafka.bootstrap.servers** | Kafka brokers to connect to.                    | Kafka                  |\n",
    "| **subscribe**           | Kafka topic(s) to subscribe.                      | Kafka (read)           |\n",
    "| **topic**               | Kafka topic to write to.                          | Kafka (write)          |\n",
    "| **startingOffsets**     | Where to start reading in Kafka (\"earliest\", \"latest\"). | Kafka (read)      |\n",
    "| **failOnDataLoss**      | Whether to fail if data loss is detected (true/false). | Kafka, Delta       |\n",
    "| **maxFilesPerTrigger**  | Max number of files to read per trigger interval. | File sources           |\n",
    "| **partitionOverwriteMode** | Overwrite behavior (\"dynamic\" is common).       | Delta, Parquet          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "567f6946-bfa1-4795-8ed1-21c03b5d040c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OUTPUT MODE\n",
    "\n",
    "By default is append\n",
    "\n",
    "| Output Mode | Description                                           | Typical Use Case                     |\n",
    "|-------------|-------------------------------------------------------|--------------------------------------|\n",
    "| **append**  | Writes only **new rows** since the last trigger.      | File sinks, Kafka, Delta (most common). |\n",
    "| **complete**| Writes **the entire result table** every time.        | Aggregations (e.g., counts, sums).   |\n",
    "| **update**  | Writes **only rows that changed** since the last trigger. | Aggregations with watermarking.      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dedf0a4-24cf-4707-bf64-fc3ac9e2fcbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TRIGGER\n",
    "\n",
    "If you dont define trigger, spark will execute a micro-batch of 1 second\n",
    "\n",
    "| Trigger Option         | Description                                              | Example                              |\n",
    "|------------------------|----------------------------------------------------------|--------------------------------------|\n",
    "| **processingTime**     | Runs micro-batches every given interval.                 | `.trigger(processingTime=\"10 seconds\")` |\n",
    "| **once**               | Runs **one batch only** then stops (like batch job).     | `.trigger(once=True)`                |\n",
    "| **availableNow**       | Runs batches until all available data is processed, then stops. | `.trigger(availableNow=True)`       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b236c50-6fd4-46ca-a51b-61cb4529de91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## EXMAPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b31a41b-b925-4f3a-9f19-8a0f0d3eda6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col , count as _count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432131fc-2301-4835-ae6e-327217707d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
    "df_aggregated = (\n",
    "  df.withWatermark(\"timestamp\", \"1 minutes\")\n",
    "  .groupBy(window(col(\"timestamp\"), \"30 seconds\"))\n",
    "  .agg(_count(\"*\").alias(\"counter\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a081148-d12c-4554-8f6d-e57fcca1ef44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CONSOLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a7a80f-d202-4c73-b624-4b9e3e304d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = df_aggregated.writeStream.format(\"console\").outputMode(\"complete\").trigger(\n",
    "    processingTime=\"10 seconds\"\n",
    ").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae3f1e6-3538-42e0-9b7d-e7ddbee50ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdca3767-161c-4309-9ec7-e04b3ea0c7bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beaa8f1b-a04a-4c45-979d-b5e9a6fd0bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52afe315-7ee6-45ff-a6dd-71357680f551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    df_aggregated.writeStream.format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .queryName(\"training\")\n",
    "    .start()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e5e6e3-e0c2-44ff-8b0b-d2b32113edf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a3c125-219c-4b01-8779-9a5fc8a03643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca2efd86-2515-4d55-9b79-f0dcf112e979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### FOR EACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f86960ee-1c6a-4fa2-a75f-753602100532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530d22fe-640a-4331-bddf-248ea914af1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_foreach(row):\n",
    "    print(f\"row     : {row}\")\n",
    "    print(f\"collect : {row.timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2d0d7f-40e3-4d17-89e6-77c8c2a79dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    df.writeStream.foreach(process_foreach)\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .queryName(\"training\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "183d0286-d04d-4da9-b0ca-0789a128472c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69183663-cfe3-4925-8ed9-ccc05c7efada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CustomSinkEach:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        \"\"\"\n",
    "            Initializes resources per partition (e.g., open DB connection).\n",
    "        \"\"\"\n",
    "        print(f\"partition_id : {partition_id}\")\n",
    "        print(f\"epoch_id     : {epoch_id}\")\n",
    "        return True # Ready to process rows\n",
    "\n",
    "    def process(self, row):\n",
    "        \"\"\"\n",
    "            Processes each row (main logic goes here).\n",
    "        \"\"\"\n",
    "        print(f\"processing row: {row}\")\n",
    "\n",
    "    def close(self, error):\n",
    "        \"\"\"\n",
    "            Cleans up resources (e.g., close DB connection), called after processing or on error.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f29113-a6fc-42e2-b575-b5a5201b2394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    df.writeStream.foreach(CustomSinkEach())\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .queryName(\"training\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82bb11dd-2b5e-4c78-94b5-bca0b193c9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### FOR EACH BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4192dd6a-be98-4b3e-ab75-1b8863fc6fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id):\n",
    "    print(f\"batch_id : {batch_id}\")\n",
    "    print(f\"counter  : {batch_df.count()}\")\n",
    "    batch_df.show()\n",
    "    batch_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4417a432-f820-4fb8-a4bf-7fe2d53315f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    df.writeStream.foreachBatch(process_batch)\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .queryName(\"training\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c44ba91-2ca0-4759-b1d3-afb5cec4ae6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901f7a53-1b44-4bac-84b9-3400ffa88310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path: str = \"/mnt/data/streaming_lab3\"\n",
    "dbutils.fs.rm(base_path, recurse=True)\n",
    "dbutils.fs.mkdirs(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bfc9607-96f0-4e15-87dc-6505be89955c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = df.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoint\") \\\n",
    "    .option(\"path\",base_path) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2bb876-6e76-4980-8d81-2d5595d8710e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.fs.ls(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c39c3bc-8490-4ea4-9ee4-42557c84ef4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TO TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b12936f-476f-4ed2-9aed-ab5a8d83ac29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoint\") \\\n",
    "    .table(\"default.streaming_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28ffc97-4bce-49fb-97d7-b5d449b7c4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM default.streaming_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe0f6375-a68e-48ea-ace8-ea1315df169b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DEDUPLICATION\n",
    "\n",
    "\n",
    "It is the process of eliminating duplicate data in a real-time data stream. This is crucial when data arrives from various sources (e.g., sensors, databases, or distributed systems) and may contain duplicate records due to network failures, retransmission attempts, or simply due to the nature of the source system.\n",
    "\n",
    "`df_aggregated.dropDuplicates([\"id_evento\", \"window\"])`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7667075370502113,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "003.write.stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
